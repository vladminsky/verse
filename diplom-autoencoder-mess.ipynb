{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автокодировщики - это класс нейронных сетей, которые пытаются реконструировать входные данные с применением обратного распространения. Автокодировщик состоит из двух частей: кодировщик и декодер. Кодировщик читает входные данные и сжимает их, порождая более компактное представление, а декодер читает это представление и пытается восстановить по нему вход. Иными словами, автокодировщик пытается обучить тождественную функцию, минимизируя ошибку реконструкции.\n",
    "На первый взгляд, тождественная функция не представляет ничего интересного, но важно, как именно производится обучение. Число скрытых слоев автокодировщика обычно меньше числа входных (и выходных) блоков. Это вынуждает кодировщик обучаться сжатому представлению входа, которое декодер реконструирует. Если входные данные обладают структурой в виде корреляций между входными признаками, то автокодировщик выявит некоторые корреляции и в итоге обучится представлению данных меньшей размерности аналогично тому, как это делается в методе главных компонент (principal component analysis, РСА)\n",
    "\n",
    "Обучив автокодировщик, декодер обычно отбрасывают и используют только кодировщик для порождения компактных представлений входных данных. Можно вместо этого использовать кодировщик как детектор признаков, порождающий компактное, семантически полноценное представление входа, и построить классификатор, присоединив к скрытому слою слой с функцией активации softmax.\n",
    "\n",
    "Ранее мы уже встречались с погружениями слов, в результате чего получается вектор, представляющий смысл слова в контексте других слов, совместно с которыми оно встречается. А сейчас мы посмотрим, как построить аналогичные векторы для предложений. Предложение - это последовательность слов, а вектор предложения представляет его смысл.\n",
    "Самый простой способ построить вектор предложения - сложить все векторы слов и поделить сумму на число слов. Но в этом случае предложение трактуется как мешок слов, и порядок слов не принимается во внимание. При таком подходе предложения The dog bit the man (Собака укусила человека) и The man bit the dog (Человек укусил собаку) считались бы идентичными. LSTM предназначена для работы с входными последовательностями и учитывает порядок слов, поэтому является более естественным представлением предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ИСТОЧНИКИ\n",
    "\n",
    "https://www.kaggle.com/tunguz/russian-glove/downloads/russian-glove.zip/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_sentences(src = \"data/poems.txt\"):\n",
    "    file_path_src = src\n",
    "    allHaiku = []\n",
    "    with open(file_path_src, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        by3lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                allHaiku.append(by3lines)\n",
    "                by3lines = []\n",
    "            else:\n",
    "                by3lines.append(line.lower())\n",
    "    return allHaiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_oneline(data):\n",
    "    return [\"\".join(row) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "    return to_oneline(stream_sentences(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = get_sentences(\"data/poems_fix.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download glove file from Kaggle to data directory\n",
    "\n",
    "https://www.kaggle.com/tunguz/russian-glove\n",
    "\n",
    "TODO: request file from shareable URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file, embed_size=300):\n",
    "    words = [\n",
    "        \"PAD\",\n",
    "        \"UNK\"]\n",
    "    vects = [\n",
    "        np.zeros((embed_size)), # PAD\n",
    "        np.random.uniform(-1, 1, embed_size)] # UNK\n",
    "\n",
    "    fglove = open(glove_file, \"rb\")\n",
    "    for line in fglove:\n",
    "        cols = line.strip().split()\n",
    "        word = cols[0].decode('utf-8')\n",
    "        vect = np.array([float(v) for v in cols[1:]])\n",
    "        words.append(word)\n",
    "        vects.append(vect)\n",
    "\n",
    "    vocab = {w: i for i, w in enumerate(words)}\n",
    "    return words, vocab, np.array(vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocab, embeddings = load_glove_vectors(\"data/multilingual_embeddings.ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63068, 300)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"data/multilingual_embeddings.ru\", word2vec_output_file=\"data/gensim_glove_vectors.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"data/gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('оборачиваетесь', 0.7298823595046997),\n",
       " ('а', 0.7228611707687378),\n",
       " ('черно', 0.654677152633667),\n",
       " ('обеспеченные', 0.6393757462501526),\n",
       " ('поднимаются', 0.6386812925338745),\n",
       " ('мерцающее', 0.6307729482650757),\n",
       " ('разбираем', 0.6296200752258301),\n",
       " ('маршировать', 0.6277832984924316),\n",
       " ('поступая', 0.6192404627799988),\n",
       " ('повторяли', 0.6133629679679871)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\"и\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'оборачиваетесь'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.similar_by_vector(embeddings[vocab[\"и\"]])[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63070, 300)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13027"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"и\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'и'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[13027]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_word2id(word, vocab):\n",
    "    return vocab[\"UNK\"] if vocab.get(word) == None else vocab[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "SEQUENCE_LEN = 20\n",
    "\n",
    "except_words = collections.Counter()\n",
    "normal_words = {}\n",
    "tokens_vectors = [[w for w in nltk.word_tokenize(s)] for s in sents]\n",
    "# verse_index_vectors = [[safe_word2id(w, vocab) for w in s] for s in tokens_vectors]\n",
    "windex_vectors = []\n",
    "good_sentences = []\n",
    "good_verses = []\n",
    "for s in tokens_vectors:\n",
    "    sent_vect = []\n",
    "    unk_count = 0\n",
    "    for w in s:\n",
    "        i = safe_word2id(w, vocab)\n",
    "        sent_vect.append(i)\n",
    "        if (i == vocab[\"UNK\"]):\n",
    "            unk_count += 1\n",
    "            except_words[w] += 1\n",
    "        else:\n",
    "            normal_words[w] = 1\n",
    "\n",
    "    windex_vectors.append(sent_vect)\n",
    "    if unk_count < 2 and s != []:\n",
    "        good_sentences.append(sent_vect)\n",
    "        good_verses.append(s)\n",
    "\n",
    "# [[safe_word2id(w, vocab) for w in s] for s in tokens_vectors]\n",
    "verse_index_vectors = sequence.pad_sequences(windex_vectors, SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1789"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/good.txt\", sequence.pad_sequences(good_sentences, SEQUENCE_LEN), delimiter=\"\\t\")\n",
    "# r = np.genfromtxt(\"data/index.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14092"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16676"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(except_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_exceptions(sents, n = 40):\n",
    "    except_words = collections.Counter()\n",
    "    normal_words = {}\n",
    "    tokens_vectors = [[w for w in nltk.word_tokenize(s)] for s in sents]\n",
    "    # verse_index_vectors = [[safe_word2id(w, vocab) for w in s] for s in tokens_vectors]\n",
    "    windex_vectors = []\n",
    "    for s in tokens_vectors:\n",
    "        sent_vect = []\n",
    "        for w in s:\n",
    "            i = safe_word2id(w, vocab)\n",
    "            sent_vect.append(i)\n",
    "            if (i == vocab[\"UNK\"]):\n",
    "                except_words[w] += 1\n",
    "            else:\n",
    "                normal_words[w] = 1\n",
    "\n",
    "        windex_vectors.append(sent_vect)\n",
    "\n",
    "    return except_words.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2index(tokens_vectors, seq_len = SEQUENCE_LEN):\n",
    "    windex_vectors = [[safe_word2id(w, vocab) for w in s] for s in tokens_vectors]\n",
    "    sequence.pad_sequences(windex_vectors, seq_len)\n",
    "\n",
    "def index2vectors(index_vectors):\n",
    "    return embeddings[index_vectors]\n",
    "\n",
    "def vectors2tokens(vectors):\n",
    "    for sent in vectors:\n",
    "        print(\" \".join([glove_model.similar_by_vector(word_vect)[0][0] for word_vect in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacex2y(x, y):\n",
    "    file_path = 'data/poems_fix.txt'\n",
    "    s = \"\"\n",
    "    with open(file_path, 'r') as myfile:\n",
    "      s = myfile.read()\n",
    "    s = s.replace(x, y)\n",
    "    with open(file_path, 'w') as myfile:\n",
    "      myfile.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2b(b, a):\n",
    "    for n in a:\n",
    "        replacex2y(\" \" + n + \" \", \" \" + b + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\"\\n\", \" \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" што \", \" что \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" говна \", \" дерьма \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" говно \", \" дерьмо \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" хуй \", \" хер \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" исус \", \" иисус \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" исуса \", \" иисуса \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" глядит \", \" смотрит \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" тихонько \", \" тихо \")\n",
    "replacex2y(\" тихонечко \", \" тихо \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" чаю \", \" чая \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" жопа \", \" опа \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" жопы \", \" мат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" жопе \", \" мат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" сука \", \" мат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" жопу \", \" мат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" жопой \", \" мат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" минет \", \" мат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" молчит \", \" молчат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" пиздец \", \" конец \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" глядят \", \" смотрят \")\n",
    "replacex2y(\" гляжу \", \" смотрю \")\n",
    "replacex2y(\" глядишь \", \" глядишь \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" ево \", \" его \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" чтото \", \" что то \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" гдето \", \" где то \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" ктото \", \" кто то \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" изза \", \" из за \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" както \", \" как то \")\n",
    "replacex2y(\" чтонибудь \", \" что нибудь \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" наутро \", \" на утро \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" иль \", \" или \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" щас \", \" сейчас \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" ещо \", \" еще \")\n",
    "replacex2y(\" умрем \", \" умрём \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" вобще \", \" вообще \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" покуда \", \" пока \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" чорных \", \" чёрных \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" чорный \", \" чёрный \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" чорные \", \" чёрные \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" чорное \", \" чёрное \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" страшней \", \" страшнее \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" рукою \", \" рукой \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacex2y(\" весною \", \" весной \")\n",
    "replacex2y(\" порою \", \" порой \")\n",
    "replacex2y(\" зовёт \", \" зовет \")\n",
    "replacex2y(\" толпою \", \" толпой \")\n",
    "replacex2y(\" своею \", \" своей \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b(\"нечаянно\", [\"невзначай\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    " 'олег',\n",
    " 'оксана',\n",
    " 'аркадий',\n",
    " 'глеб',\n",
    " 'олега',\n",
    " 'николая',\n",
    " 'геннадий',\n",
    " 'евгений',\n",
    " 'семён',\n",
    " 'петра',\n",
    " 'зухра',\n",
    " 'оксану',\n",
    " 'илья',\n",
    " 'глеба',\n",
    " 'олегу',\n",
    " 'оксане',\n",
    " 'иван',\n",
    " 'семен',\n",
    " 'ольга',\n",
    " 'анатолий',\n",
    " 'зульфия',\n",
    " 'оксаны',\n",
    " 'пётр',\n",
    " 'зоя',\n",
    " 'руфь',\n",
    " 'вениамин',\n",
    " 'антон',\n",
    " 'путин',\n",
    " 'петру',\n",
    " 'шаинский',\n",
    " 'игорь',\n",
    " 'андрей',\n",
    " 'семёна',\n",
    " 'пушкин',\n",
    " 'гагарин',\n",
    " 'павел',\n",
    " 'зухры',\n",
    " 'илье',\n",
    " 'оксаной',\n",
    " 'ильич',\n",
    " 'константин',\n",
    " 'иннокентий',\n",
    " 'николаем',\n",
    " 'зинаида',\n",
    " 'зульфии',\n",
    " 'зухре',\n",
    " 'ольги',\n",
    " 'николаю',\n",
    " 'олегом',\n",
    " 'зои',\n",
    " 'петрович',\n",
    " 'аделаида',\n",
    " 'марина',\n",
    " 'афанасий',\n",
    " 'глебу',\n",
    " 'зульфию',\n",
    " 'лариса',\n",
    " 'василий',\n",
    " 'петр',\n",
    " 'муму',\n",
    " 'татьяна',\n",
    " 'зое',\n",
    " 'ольгу',\n",
    " 'ильи',\n",
    " 'серёжа',\n",
    " 'маша',\n",
    " 'юра',\n",
    " 'друзь',\n",
    " 'михаил',\n",
    " 'герасим',\n",
    " 'игоря',\n",
    " 'ленин',\n",
    " 'аркадию',\n",
    " 'илью',\n",
    " 'андрея',\n",
    " 'ильича',\n",
    " 'саша',\n",
    " 'эммануил',\n",
    " 'зухру',\n",
    " 'иуда',\n",
    " 'фёдор',\n",
    " 'боярский',\n",
    " 'олеге',\n",
    " 'петыр',\n",
    " 'алексей',\n",
    " 'ольгой',\n",
    " 'кобзон',\n",
    " 'коля',\n",
    " 'оксан',\n",
    " 'ивана',\n",
    "]\n",
    "\n",
    "for n in names:\n",
    "    replacex2y(\" \" + n + \" \", \" имя \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    " 'борщ',\n",
    " 'пельмени',\n",
    " 'водку',\n",
    " 'пирожки',\n",
    " 'пирожок',\n",
    " 'пельменей',\n",
    " 'омлет',\n",
    " 'кастрюлю',\n",
    " 'борща',\n",
    " 'солью',\n",
    " 'колбасы',\n",
    " 'арбузы',\n",
    " 'пельменя',\n",
    " 'пельмень',\n",
    " 'котлеты',\n",
    "]\n",
    "\n",
    "for n in names:\n",
    "    replacex2y(\" \" + n + \" \", \" еда \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b(\"умер\", [\n",
    "    \"помер\",\n",
    "    \"сдох\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b(\"мат\", [\n",
    "    \"срать\",\n",
    "    \"мудак\",\n",
    "    \"блядь\",\n",
    "    \"блять\",\n",
    "    \"попу\",\n",
    "    \"говном\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b(\"коробка\", [\"гроб\"])\n",
    "a2b(\"коробки\", [\"гроба\", \"гробы\"])\n",
    "a2b(\"могилы\", [\"могил\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['седой',\n",
       " 'седая',\n",
       " 'рублей',\n",
       " 'жених',\n",
       " 'косой',\n",
       " 'семнадцать',\n",
       " 'ночами',\n",
       " 'подъезда',\n",
       " 'крикнул',\n",
       " 'лужи',\n",
       " 'москву',\n",
       " 'горько',\n",
       " 'приснился',\n",
       " 'сантехник',\n",
       " 'снится',\n",
       " 'пятьсот',\n",
       " 'трамвае',\n",
       " 'ежа',\n",
       " 'усталый',\n",
       " 'гладит',\n",
       " 'вибратор',\n",
       " 'зонт',\n",
       " 'воет',\n",
       " 'подъезде',\n",
       " 'трамвай',\n",
       " 'тапки',\n",
       " 'молчу',\n",
       " 'курю',\n",
       " 'украдкой',\n",
       " 'попробуй',\n",
       " 'запел',\n",
       " 'плюнул',\n",
       " 'шорох',\n",
       " 'трусах',\n",
       " 'москва',\n",
       " 'горлу',\n",
       " 'девчонка',\n",
       " 'палач',\n",
       " 'шагает',\n",
       " 'крым',\n",
       " 'срывает',\n",
       " 'целовал',\n",
       " 'мордой',\n",
       " 'трогают',\n",
       " 'кончилась',\n",
       " 'енота',\n",
       " 'лежим',\n",
       " 'бомж',\n",
       " 'раскинув',\n",
       " 'убей',\n",
       " 'уставший',\n",
       " 'трамвая',\n",
       " 'уснул',\n",
       " 'воскрес',\n",
       " 'журавлей',\n",
       " 'старенький',\n",
       " 'гуляет',\n",
       " 'колготки',\n",
       " 'бредёт',\n",
       " 'лежишь',\n",
       " 'промолвил',\n",
       " 'четырнадцать',\n",
       " 'поцеловал',\n",
       " 'ползи',\n",
       " 'дорешал',\n",
       " 'стучится',\n",
       " 'утру',\n",
       " 'трахаться',\n",
       " 'плаще',\n",
       " 'ёж',\n",
       " 'портфель',\n",
       " 'бухло',\n",
       " 'пляшут',\n",
       " 'крематорий',\n",
       " 'обнявшись',\n",
       " 'трусов',\n",
       " 'буратино',\n",
       " 'тёплый',\n",
       " 'цыганка',\n",
       " 'дедмороза',\n",
       " 'весенний',\n",
       " 'гладить',\n",
       " 'ждёшь',\n",
       " 'глядишь',\n",
       " 'буркнул',\n",
       " 'село',\n",
       " 'речке',\n",
       " 'слепил',\n",
       " 'кладет',\n",
       " 'оделся',\n",
       " 'прохожим',\n",
       " 'тугие',\n",
       " 'луже',\n",
       " 'сунул',\n",
       " 'патроны',\n",
       " 'прораб',\n",
       " 'вздохнул',\n",
       " 'ладоней',\n",
       " 'зонтом',\n",
       " 'шторы',\n",
       " 'краснея',\n",
       " 'молчали',\n",
       " 'мокрый',\n",
       " 'насовсем',\n",
       " 'август',\n",
       " 'ползет',\n",
       " 'повесился',\n",
       " 'слюной',\n",
       " 'курил',\n",
       " 'сову',\n",
       " 'осенний',\n",
       " 'бэ',\n",
       " 'тум',\n",
       " 'невесту',\n",
       " 'свадебный',\n",
       " 'молчал',\n",
       " 'ветвях',\n",
       " 'вяло',\n",
       " 'заборе',\n",
       " 'тычет',\n",
       " 'кузьмича',\n",
       " 'валялся',\n",
       " 'тёмный',\n",
       " 'поспать',\n",
       " 'орал',\n",
       " 'тошнит',\n",
       " 'пал',\n",
       " 'закрыв',\n",
       " 'привычке',\n",
       " 'хомяк',\n",
       " 'бабка',\n",
       " 'трёт',\n",
       " 'вечерний',\n",
       " 'петербург',\n",
       " 'мужиков',\n",
       " 'хрустальный',\n",
       " 'тревогой',\n",
       " 'ложкой',\n",
       " 'ведром',\n",
       " 'дрожат',\n",
       " 'выпью',\n",
       " 'угрюмый',\n",
       " 'твердил',\n",
       " 'дачу',\n",
       " 'клоун',\n",
       " 'огонёк',\n",
       " 'раздеваться',\n",
       " 'бойцы',\n",
       " 'углам',\n",
       " 'шестнадцать']"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x[0], get_common_exceptions(get_sentences(\"data/poems_fix.txt\"), 150)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50816"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"ну\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7509, 20)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_index_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/index.txt\", verse_index_vectors, delimiter=\"\\t\")\n",
    "# r = np.genfromtxt(\"data/index.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель автокодировщика принимает последовательность GloVe-векторов слов и обучается порождать другую последовательность, похожую на входную. LSTM-кодировщик сжимает последовательность в контекстный вектор фиксированной длины, по которой LSTM-декодер реконструирует исходную последовательность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division, print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msin\n",
    "def compute_cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x, 2) * np.linalg.norm(y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences:  7509\n",
      "(5256, 20) (2253, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# split sentences into training and test\n",
    "train_size = 0.7\n",
    "Xtrain, Xtest = train_test_split(verse_index_vectors, train_size=train_size)\n",
    "print(\"number of sentences: \", len(verse_index_vectors))\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generator(X, embeddings, batch_size):\n",
    "    while True:\n",
    "        # loop once per epoch\n",
    "        num_recs = X.shape[0]\n",
    "        indices = np.random.permutation(np.arange(num_recs))\n",
    "        num_batches = num_recs // batch_size\n",
    "        for bid in range(num_batches):\n",
    "            sids = indices[bid * batch_size: (bid + 1) * batch_size]\n",
    "            Xbatch = embeddings[X[sids, :]]\n",
    "            yield Xbatch, Xbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63070, 300)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5256, 20)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2253, 20)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and test generators\n",
    "BATCH_SIZE = 64\n",
    "train_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)\n",
    "test_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals\n",
    "from time import gmtime, strftime\n",
    "from keras.callbacks import TensorBoard\n",
    "import os\n",
    "\n",
    "\n",
    "def make_tensorboard(set_dir_name=''):\n",
    "    tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())\n",
    "    directory_name = tictoc\n",
    "    log_dir = set_dir_name + '_' + directory_name\n",
    "    os.mkdir(log_dir)\n",
    "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, )\n",
    "    return tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define autoencoder network\n",
    "EMBED_SIZE = 300\n",
    "LATENT_SIZE = 512\n",
    "\n",
    "inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=\"input\")\n",
    "encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=\"sum\",\n",
    "                        name=\"encoder_lstm\")(inputs)\n",
    "\n",
    "decoded = RepeatVector(SEQUENCE_LEN, name=\"repeater\")(encoded)\n",
    "decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True),\n",
    "                        merge_mode=\"sum\",\n",
    "                        name=\"decoder_lstm\")(decoded)\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "\n",
    "tensorboard = make_tensorboard(set_dir_name='rnn')\n",
    "\n",
    "autoencoder.compile(optimizer=\"sgd\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "82/82 [==============================] - 49s 599ms/step - loss: 0.1936 - val_loss: 0.1939\n",
      "Epoch 2/100\n",
      "82/82 [==============================] - 47s 576ms/step - loss: 0.1900 - val_loss: 0.1900\n",
      "Epoch 3/100\n",
      "82/82 [==============================] - 46s 559ms/step - loss: 0.1880 - val_loss: 0.1887\n",
      "Epoch 4/100\n",
      "82/82 [==============================] - 46s 565ms/step - loss: 0.1857 - val_loss: 0.1865\n",
      "Epoch 5/100\n",
      "82/82 [==============================] - 46s 563ms/step - loss: 0.1840 - val_loss: 0.1848\n",
      "Epoch 6/100\n",
      "82/82 [==============================] - 47s 569ms/step - loss: 0.1820 - val_loss: 0.1830\n",
      "Epoch 7/100\n",
      "82/82 [==============================] - 47s 569ms/step - loss: 0.1807 - val_loss: 0.1821\n",
      "Epoch 8/100\n",
      "82/82 [==============================] - 47s 570ms/step - loss: 0.1805 - val_loss: 0.1798\n",
      "Epoch 9/100\n",
      "82/82 [==============================] - 47s 569ms/step - loss: 0.1789 - val_loss: 0.1796\n",
      "Epoch 10/100\n",
      "82/82 [==============================] - 46s 563ms/step - loss: 0.1776 - val_loss: 0.1808\n",
      "Epoch 11/100\n",
      "82/82 [==============================] - 46s 566ms/step - loss: 0.1777 - val_loss: 0.1788\n",
      "Epoch 12/100\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.1759 - val_loss: 0.1763\n",
      "Epoch 13/100\n",
      "82/82 [==============================] - 47s 578ms/step - loss: 0.1755 - val_loss: 0.1760\n",
      "Epoch 14/100\n",
      "82/82 [==============================] - 48s 591ms/step - loss: 0.1758 - val_loss: 0.1780\n",
      "Epoch 15/100\n",
      "82/82 [==============================] - 50s 605ms/step - loss: 0.1742 - val_loss: 0.1753\n",
      "Epoch 16/100\n",
      "82/82 [==============================] - 51s 624ms/step - loss: 0.1739 - val_loss: 0.1767\n",
      "Epoch 17/100\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.1730 - val_loss: 0.1755\n",
      "Epoch 18/100\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.1734 - val_loss: 0.1743\n",
      "Epoch 19/100\n",
      "82/82 [==============================] - 51s 617ms/step - loss: 0.1732 - val_loss: 0.1744\n",
      "Epoch 20/100\n",
      "82/82 [==============================] - 50s 606ms/step - loss: 0.1719 - val_loss: 0.1730\n",
      "Epoch 21/100\n",
      "82/82 [==============================] - 48s 585ms/step - loss: 0.1725 - val_loss: 0.1713\n",
      "Epoch 22/100\n",
      "82/82 [==============================] - 48s 580ms/step - loss: 0.1718 - val_loss: 0.1760\n",
      "Epoch 23/100\n",
      "82/82 [==============================] - 48s 581ms/step - loss: 0.1714 - val_loss: 0.1720\n",
      "Epoch 24/100\n",
      "82/82 [==============================] - 51s 619ms/step - loss: 0.1712 - val_loss: 0.1726\n",
      "Epoch 25/100\n",
      "82/82 [==============================] - 49s 600ms/step - loss: 0.1707 - val_loss: 0.1721\n",
      "Epoch 26/100\n",
      "82/82 [==============================] - 50s 612ms/step - loss: 0.1711 - val_loss: 0.1735\n",
      "Epoch 27/100\n",
      "82/82 [==============================] - 47s 579ms/step - loss: 0.1701 - val_loss: 0.1714\n",
      "Epoch 28/100\n",
      "82/82 [==============================] - 51s 627ms/step - loss: 0.1702 - val_loss: 0.1718\n",
      "Epoch 29/100\n",
      "82/82 [==============================] - 47s 573ms/step - loss: 0.1705 - val_loss: 0.1718\n",
      "Epoch 30/100\n",
      "82/82 [==============================] - 46s 566ms/step - loss: 0.1710 - val_loss: 0.1723\n",
      "Epoch 31/100\n",
      "82/82 [==============================] - 47s 567ms/step - loss: 0.1697 - val_loss: 0.1702\n",
      "Epoch 32/100\n",
      "82/82 [==============================] - 48s 589ms/step - loss: 0.1700 - val_loss: 0.1737\n",
      "Epoch 33/100\n",
      "82/82 [==============================] - 50s 605ms/step - loss: 0.1699 - val_loss: 0.1705\n",
      "Epoch 34/100\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 0.1699 - val_loss: 0.1715\n",
      "Epoch 35/100\n",
      "82/82 [==============================] - 50s 609ms/step - loss: 0.1688 - val_loss: 0.1716\n",
      "Epoch 36/100\n",
      "82/82 [==============================] - 49s 600ms/step - loss: 0.1697 - val_loss: 0.1701\n",
      "Epoch 37/100\n",
      "82/82 [==============================] - 50s 604ms/step - loss: 0.1691 - val_loss: 0.1713\n",
      "Epoch 38/100\n",
      "82/82 [==============================] - 49s 601ms/step - loss: 0.1695 - val_loss: 0.1711\n",
      "Epoch 39/100\n",
      "82/82 [==============================] - 49s 599ms/step - loss: 0.1691 - val_loss: 0.1713\n",
      "Epoch 40/100\n",
      "82/82 [==============================] - 51s 620ms/step - loss: 0.1691 - val_loss: 0.1698\n",
      "Epoch 41/100\n",
      "82/82 [==============================] - 49s 601ms/step - loss: 0.1689 - val_loss: 0.1709\n",
      "Epoch 42/100\n",
      "82/82 [==============================] - 50s 607ms/step - loss: 0.1687 - val_loss: 0.1716\n",
      "Epoch 43/100\n",
      "82/82 [==============================] - 49s 600ms/step - loss: 0.1689 - val_loss: 0.1689\n",
      "Epoch 44/100\n",
      "82/82 [==============================] - 50s 613ms/step - loss: 0.1693 - val_loss: 0.1717\n",
      "Epoch 45/100\n",
      "82/82 [==============================] - 50s 609ms/step - loss: 0.1683 - val_loss: 0.1703\n",
      "Epoch 46/100\n",
      "82/82 [==============================] - 51s 625ms/step - loss: 0.1681 - val_loss: 0.1691\n",
      "Epoch 47/100\n",
      "82/82 [==============================] - 50s 613ms/step - loss: 0.1693 - val_loss: 0.1711\n",
      "Epoch 48/100\n",
      "82/82 [==============================] - 50s 608ms/step - loss: 0.1686 - val_loss: 0.1706\n",
      "Epoch 49/100\n",
      "82/82 [==============================] - 50s 605ms/step - loss: 0.1680 - val_loss: 0.1696\n",
      "Epoch 50/100\n",
      "82/82 [==============================] - 47s 576ms/step - loss: 0.1691 - val_loss: 0.1702\n",
      "Epoch 51/100\n",
      "82/82 [==============================] - 52s 638ms/step - loss: 0.1685 - val_loss: 0.1692\n",
      "Epoch 52/100\n",
      "82/82 [==============================] - 47s 577ms/step - loss: 0.1680 - val_loss: 0.1693\n",
      "Epoch 53/100\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 0.1687 - val_loss: 0.1709\n",
      "Epoch 54/100\n",
      "82/82 [==============================] - 47s 579ms/step - loss: 0.1681 - val_loss: 0.1695\n",
      "Epoch 55/100\n",
      "82/82 [==============================] - 48s 581ms/step - loss: 0.1678 - val_loss: 0.1699\n",
      "Epoch 56/100\n",
      "82/82 [==============================] - 50s 609ms/step - loss: 0.1684 - val_loss: 0.1700\n",
      "Epoch 57/100\n",
      "82/82 [==============================] - 51s 622ms/step - loss: 0.1680 - val_loss: 0.1684\n",
      "Epoch 58/100\n",
      "82/82 [==============================] - 52s 636ms/step - loss: 0.1682 - val_loss: 0.1713\n",
      "Epoch 59/100\n",
      "82/82 [==============================] - 53s 647ms/step - loss: 0.1685 - val_loss: 0.1683\n",
      "Epoch 60/100\n",
      "82/82 [==============================] - 49s 603ms/step - loss: 0.1671 - val_loss: 0.1712\n",
      "Epoch 61/100\n",
      "82/82 [==============================] - 52s 634ms/step - loss: 0.1687 - val_loss: 0.1687\n",
      "Epoch 62/100\n",
      "82/82 [==============================] - 48s 591ms/step - loss: 0.1676 - val_loss: 0.1691\n",
      "Epoch 63/100\n",
      "82/82 [==============================] - 48s 590ms/step - loss: 0.1674 - val_loss: 0.1690\n",
      "Epoch 64/100\n",
      "82/82 [==============================] - 49s 599ms/step - loss: 0.1682 - val_loss: 0.1698\n",
      "Epoch 65/100\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.1679 - val_loss: 0.1702\n",
      "Epoch 66/100\n",
      "82/82 [==============================] - 48s 591ms/step - loss: 0.1675 - val_loss: 0.1683\n",
      "Epoch 67/100\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.1681 - val_loss: 0.1697\n",
      "Epoch 68/100\n",
      "82/82 [==============================] - 50s 609ms/step - loss: 0.1673 - val_loss: 0.1700\n",
      "Epoch 69/100\n",
      "82/82 [==============================] - 47s 574ms/step - loss: 0.1675 - val_loss: 0.1698\n",
      "Epoch 70/100\n",
      "82/82 [==============================] - 48s 589ms/step - loss: 0.1679 - val_loss: 0.1681\n",
      "Epoch 71/100\n",
      "82/82 [==============================] - 49s 601ms/step - loss: 0.1677 - val_loss: 0.1695\n",
      "Epoch 72/100\n",
      "82/82 [==============================] - 50s 604ms/step - loss: 0.1675 - val_loss: 0.1695\n",
      "Epoch 73/100\n",
      "82/82 [==============================] - 48s 584ms/step - loss: 0.1668 - val_loss: 0.1686\n",
      "Epoch 74/100\n",
      "82/82 [==============================] - 47s 578ms/step - loss: 0.1676 - val_loss: 0.1687\n",
      "Epoch 75/100\n",
      "82/82 [==============================] - 48s 588ms/step - loss: 0.1677 - val_loss: 0.1697\n",
      "Epoch 76/100\n",
      "82/82 [==============================] - 50s 606ms/step - loss: 0.1674 - val_loss: 0.1693\n",
      "Epoch 77/100\n",
      "82/82 [==============================] - 50s 606ms/step - loss: 0.1672 - val_loss: 0.1686\n",
      "Epoch 78/100\n",
      "82/82 [==============================] - 49s 592ms/step - loss: 0.1673 - val_loss: 0.1681\n",
      "Epoch 79/100\n",
      "82/82 [==============================] - 47s 576ms/step - loss: 0.1674 - val_loss: 0.1691\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 48s 585ms/step - loss: 0.1674 - val_loss: 0.1692\n",
      "Epoch 81/100\n",
      "82/82 [==============================] - 49s 595ms/step - loss: 0.1668 - val_loss: 0.1684\n",
      "Epoch 82/100\n",
      "82/82 [==============================] - 48s 582ms/step - loss: 0.1676 - val_loss: 0.1697\n",
      "Epoch 83/100\n",
      "82/82 [==============================] - 47s 579ms/step - loss: 0.1666 - val_loss: 0.1675\n",
      "Epoch 84/100\n",
      "82/82 [==============================] - 48s 583ms/step - loss: 0.1670 - val_loss: 0.1704\n",
      "Epoch 85/100\n",
      "82/82 [==============================] - 47s 578ms/step - loss: 0.1677 - val_loss: 0.1675\n",
      "Epoch 86/100\n",
      "82/82 [==============================] - 47s 576ms/step - loss: 0.1664 - val_loss: 0.1688\n",
      "Epoch 87/100\n",
      "82/82 [==============================] - 49s 603ms/step - loss: 0.1678 - val_loss: 0.1681\n",
      "Epoch 88/100\n",
      "82/82 [==============================] - 48s 582ms/step - loss: 0.1662 - val_loss: 0.1696\n",
      "Epoch 89/100\n",
      "82/82 [==============================] - 49s 595ms/step - loss: 0.1669 - val_loss: 0.1681\n",
      "Epoch 90/100\n",
      "82/82 [==============================] - 50s 612ms/step - loss: 0.1673 - val_loss: 0.1694\n",
      "Epoch 91/100\n",
      "82/82 [==============================] - 49s 603ms/step - loss: 0.1663 - val_loss: 0.1675\n",
      "Epoch 92/100\n",
      "82/82 [==============================] - 46s 558ms/step - loss: 0.1675 - val_loss: 0.1687\n",
      "Epoch 93/100\n",
      "82/82 [==============================] - 48s 580ms/step - loss: 0.1668 - val_loss: 0.1682\n",
      "Epoch 94/100\n",
      "82/82 [==============================] - 47s 570ms/step - loss: 0.1666 - val_loss: 0.1684\n",
      "Epoch 95/100\n",
      "82/82 [==============================] - 48s 582ms/step - loss: 0.1671 - val_loss: 0.1686\n",
      "Epoch 96/100\n",
      "82/82 [==============================] - 48s 586ms/step - loss: 0.1665 - val_loss: 0.1678\n",
      "Epoch 97/100\n",
      "82/82 [==============================] - 47s 578ms/step - loss: 0.1666 - val_loss: 0.1681\n",
      "Epoch 98/100\n",
      "82/82 [==============================] - 47s 572ms/step - loss: 0.1670 - val_loss: 0.1697\n",
      "Epoch 99/100\n",
      "82/82 [==============================] - 47s 572ms/step - loss: 0.1665 - val_loss: 0.1677\n",
      "Epoch 100/100\n",
      "82/82 [==============================] - 46s 557ms/step - loss: 0.1671 - val_loss: 0.1693\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "NUM_EPOCHS = 100\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "num_train_steps = len(Xtrain) // BATCH_SIZE\n",
    "num_test_steps = len(Xtest) // BATCH_SIZE\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(DATA_DIR, \"sent-thoughts-autoencoder.h5\"),\n",
    "    save_best_only=True)\n",
    "\n",
    "history = autoencoder.fit_generator(train_gen,\n",
    "                                    steps_per_epoch=num_train_steps,\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    validation_data=test_gen,\n",
    "                                    validation_steps=num_test_steps,\n",
    "                                    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17613"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain[Xtrain == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract encoder model\n",
    "encoder = Model(autoencoder.input, autoencoder.get_layer(\"encoder_lstm\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collect autoencoder predictions for test set\n",
    "test_inputs, test_labels = next(test_gen)\n",
    "preds = autoencoder.predict(test_inputs)\n",
    "\n",
    "# compute difference between vector produced by original and autoencoded\n",
    "k = 500\n",
    "cosims = np.zeros((k))\n",
    "i = 0\n",
    "for bid in range(num_test_steps):\n",
    "    xtest, _ = next(test_gen)\n",
    "    ytest = autoencoder.predict(xtest)\n",
    "    Xvec = encoder.predict(xtest)\n",
    "    Yvec = encoder.predict(ytest)\n",
    "    for rid in range(Xvec.shape[0]):\n",
    "        if i >= k:\n",
    "            break\n",
    "        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])\n",
    "        i += 1\n",
    "    if i >= k:\n",
    "        break\n",
    "\n",
    "plt.hist(cosims, bins=10, normed=True)\n",
    "plt.xlabel(\"cosine similarity\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    \n",
    "    z_dim = (LATENT_SIZE,)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Hidden layer\n",
    "    model.add(Dense(LATENT_SIZE * 2, input_dim=LATENT_SIZE))\n",
    "\n",
    "    # Leaky ReLU\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(Dense(SEQUENCE_LEN * EMBED_SIZE, activation='sigmoid'))\n",
    "    model.add(Reshape((SEQUENCE_LEN, EMBED_SIZE)))\n",
    "\n",
    "    x = Input(shape=z_dim)\n",
    "    y = model(x)\n",
    "    \n",
    "    return Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=\"input\")\n",
    "    encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=\"sum\", name=\"encoder_lstm\")(inputs)\n",
    "    \n",
    "    # Hidden layer\n",
    "    y = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    # Leaky ReLU\n",
    "#     model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "#     decoded = RepeatVector(SEQUENCE_LEN, name=\"repeater\")(encoded)\n",
    "#     decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True), merge_mode=\"sum\", name=\"decoder_lstm\")(decoded)\n",
    "\n",
    "#     encoded\n",
    "#     autoencoder = Model(inputs, decoded)\n",
    "\n",
    "    return Model(inputs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the Discriminator\n",
    "discr = discriminator()\n",
    "discr.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Build the Generator\n",
    "generator = generator()\n",
    "\n",
    "# Generated image to be used as input\n",
    "z = Input(shape=(LATENT_SIZE,))\n",
    "img = generator(z)\n",
    "\n",
    "# Keep Discriminator’s parameters constant during Generator training\n",
    "# trainable = False should be set before compile\n",
    "discr.trainable = False\n",
    "\n",
    "# The Discriminator’s prediction\n",
    "prediction = discr(img)\n",
    "\n",
    "# Combined GAN model to train the Generator\n",
    "combined = Model(z, prediction)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "def train(X_train, iterations, batch_size, sample_interval):\n",
    "    # Labels for real and fake examples\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # -------------------------\n",
    "        #  Train the Discriminator\n",
    "        # -------------------------\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_imgs = X_train[idx] # Select a random batch of real images\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, LATENT_SIZE))\n",
    "        fake_imgs = generator.predict(z) # Generate a batch of fake images\n",
    "\n",
    "        # Discriminator loss\n",
    "        d_loss_real = discr.train_on_batch(real_imgs, real)\n",
    "        d_loss_fake = discr.train_on_batch(fake_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train the Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Generate a batch of fake images\n",
    "        z = np.random.normal(0, 1, (batch_size, LATENT_SIZE))\n",
    "\n",
    "        # Generator loss\n",
    "        g_loss = combined.train_on_batch(z, real)\n",
    "\n",
    "        if iteration % sample_interval == 0:\n",
    "            \n",
    "            # Output training progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % \n",
    "                         (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            \n",
    "            # Save losses and accuracies so they can be plotted after training\n",
    "            losses.append((d_loss[0], g_loss))\n",
    "            accuracies.append(100*d_loss[1])\n",
    "\n",
    "            # Output generated image samples \n",
    "            samples(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(iteration):\n",
    "\n",
    "    # Sample random noise\n",
    "    z = np.random.normal(0, 1, (1, LATENT_SIZE))\n",
    "\n",
    "    # Generate images from random noise\n",
    "    gen_imgs = generator.predict(z)\n",
    "    \n",
    "    print(gen_imgs.shape)\n",
    "    print(\" \".join([glove_model.similar_by_vector(v)[1][0] for v in gen_imgs[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5256, 20, 300)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[Xtrain].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "антарктики удовлетворял удовлетворял удовлетворял круглосуточно законам каких прошлое достижения прошлое пиломатериалы пиломатериалы демократией демократией возникающих ничего отрастит возникающих подобное заменили\n",
      "100 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "подали удовлетворял удовлетворял четко круглосуточно законам законам прошлое им прошлое проблемные миниатюрные демократией синтезаторе демократией миниатюрные возникающих перерывы подобное заменили\n",
      "200 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял круглосуточно удовлетворял удовлетворял каких прошлое достижения прошлое проблемные миниатюрные демократией синтезаторе эффективен демократией возникающих возникающих подобное заменили\n",
      "300 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "вынесения предпринял удовлетворял прошлым круглосуточно способно каких каких достижения прошлое пиломатериалы пиломатериалы демократией синтезаторе эффективен возникающих отрастит вычета вычета заменили\n",
      "400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "вынесения удовлетворял удовлетворял четко удовлетворял законам законам каких достижения прошлое проблемные миниатюрные демократией синтезаторе миниатюрные возникающих ничего вычета вычета заменили\n",
      "500 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "вынесения удовлетворял удовлетворял четко круглосуточно способно законам достижения достижения прошлое законам миниатюрные демократией синтезаторе возникающих возникающих отрастит отрастит вычета заменили\n",
      "600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял удовлетворял удовлетворял законам каких достижения достижения прошлое законам миниатюрные демократией синтезаторе демократией возникающих ничего вычета подобное заменили\n",
      "700 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "подали удовлетворял удовлетворял удовлетворял прошлым прошлое каких каких достижения прошлое банковские моралью демократией демократией эффективен ничего отрастит отрастит подобное заменили\n",
      "800 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял четко прошлым способно каких каких им прошлое значимое миниатюрные демократией синтезаторе эффективен ничего отрастит возникающих подобное угрожать\n",
      "900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "технически удовлетворял удовлетворял удовлетворял удовлетворял противостоять достижения достижения им прошлое скидку миниатюрные демократией синтезаторе миниатюрные ничего отрастит отрастит подобное угрожать\n",
      "1000 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял удовлетворял обещания способно каких прошлое им прошлое законам миниатюрные демократией синтезаторе эффективен миниатюрные отрастит отрастит подобное заменили\n",
      "1100 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "подали удовлетворял удовлетворял удовлетворял круглосуточно прошлое достижения каких достижения прошлое банковские миниатюрные демократией демократией миниатюрные возникающих отрастит вычета подобное заменили\n",
      "1200 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял четко прошлым законам каких каких достижения прошлое проблемные миниатюрные демократией демократией миниатюрные ничего возникающих возникающих подобное магистралей\n",
      "1300 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял прошлым прошлым способно законам занимаются достижения прошлое законам миниатюрные демократией синтезаторе эффективен возникающих отрастит отрастит подобное заменили\n",
      "1400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "вынесения удовлетворял удовлетворял удовлетворял способно законам каких каких им прошлое проблемные пиломатериалы демократией синтезаторе эффективен возникающих отрастит возникающих подобное заменили\n",
      "1500 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял самолётов четко круглосуточно противостоять каких достижения достижения прошлое законам миниатюрные демократией синтезаторе возникающих ничего отрастит отрастит вычета магистралей\n",
      "1600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "предпринял удовлетворял удовлетворял прошлым круглосуточно законам каких занимаются отрастит прошлое скидку пиломатериалы демократией синтезаторе эффективен миниатюрные отрастит возникающих подобное магистралей\n",
      "1700 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "вынесения удовлетворял удовлетворял удовлетворял круглосуточно законам законам каких достижения прошлое значимое миниатюрные демократией синтезаторе возникающих возникающих отрастит возникающих вычета магистралей\n",
      "1800 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "подали предпринял удовлетворял удовлетворял круглосуточно законам каких занимаются достижения прошлое проблемные моралью демократией демократией миниатюрные миниатюрные отрастит возникающих подобное магистралей\n",
      "1900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "(1, 20, 300)\n",
      "вынесения удовлетворял удовлетворял четко обещания законам каких каких достижения прошлое пиломатериалы миниатюрные демократией демократией возникающих возникающих ничего отрастит подобное заменили\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings because the warning Keras gives us about non-trainable parameters is by design:\n",
    "# The Generator trainable parameters are intentionally held constant during Discriminator training and vice versa\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "iterations = 2000\n",
    "batch_size = 64\n",
    "sample_interval = 100\n",
    "\n",
    "# Rescale -1 to 1\n",
    "X_train = embeddings[Xtrain]\n",
    "\n",
    "# Train the GAN for the specified number of iterations\n",
    "train(X_train, iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
